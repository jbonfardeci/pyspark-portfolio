{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a02aa179-a9b9-418d-829c-d381eafa9fe3",
   "metadata": {},
   "source": [
    "# Spark NER with John Snow Labs\n",
    "Author: John Bonfardeci<br/>\n",
    "Last Modified: 2021-05-25<br/>\n",
    "This notebook documents the experiments and results of various methods for named entity recognition (NER) and the spellchecking of entities identified in a sample DPAA document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hundred-assumption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.21.207.82:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DPAA NLP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6d0c8d3910>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *              \n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "from sparknlp import Finisher\n",
    "import sparknlp\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DPAA NLP\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\",\"32G\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.0.3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#sql_ctx = SQLContext(spark.sparkContext)\n",
    "\n",
    "sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f857cc9-92ce-45ae-8ebd-60fe72174e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.3'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparknlp.version()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96226dfa-498a-4a71-a2bc-c03087dd46e4",
   "metadata": {},
   "source": [
    "## Spark OCR\n",
    "https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/5.Spark_OCR.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09992911-c26e-483e-b28f-81c04ccdf2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../spark_ocr.json') as f:\n",
    "    license_keys = json.load(f)\n",
    "\n",
    "os.environ['JSL_OCR_LICENSE'] = license_keys['SPARK_OCR_LICENSE']\n",
    "ocr_license = license_keys['SPARK_OCR_LICENSE']\n",
    "ocr_secret = license_keys['JSL_OCR_SECRET'] \n",
    "ocr_version = license_keys['OCR_VERSION']\n",
    "\n",
    "! pip install --upgrade spark-ocr==$ocr_version --user --extra-index-url=https://pypi.johnsnowlabs.com/$ocr_license"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441476ed-05db-462d-ba06-ad38edac5ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports from Spark OCR library\n",
    "from sparkocr import start\n",
    "from sparkocr.transformers import *\n",
    "from sparkocr.enums import *\n",
    "from sparkocr.utils import display_image, to_pil_image\n",
    "from sparkocr.metrics import score\n",
    "import pkg_resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6fa105-33ea-4548-9d18-59022c235fae",
   "metadata": {},
   "source": [
    "## Import and Clean Sample Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "respiratory-binding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|searchable_pdf/do...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_DATA_PATH = './ner/REDACTED.txt'\n",
    "sample_text = open(SAMPLE_DATA_PATH, 'r').read()\n",
    "sample_text = re.sub(r'(\\r\\n|\\n)', ' ', sample_text)\n",
    "sample_text = re.sub(r',', '; ', sample_text)\n",
    "sample_text = re.sub(r'\\s+', ' ', sample_text)\n",
    "sample_text = re.sub(r'(?<=[^\\d])\\-(\\s+|)(?=[^\\d])', '', sample_text)\n",
    "df= pd.DataFrame({'text': sample_text}, index=[0])\n",
    "spark_df = spark.createDataFrame(df)\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950cf03c-eb23-4da0-b8e2-084fab457f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exploder(Transformer):\n",
    "    \"\"\"\n",
    "    A custom transformer that inherits the Transformer class.\n",
    "    This class explodes an array into a Spark dataframe column.\n",
    "    \"\"\"\n",
    "    inputCol=None\n",
    "    outputCol=None\n",
    "    \n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(Transformer).__init__()\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "    \n",
    "    def _transform(self, spark_frame):\n",
    "        col_name = self.inputCol\n",
    "        out_col = self.outputCol\n",
    "        return spark_frame.select([col_name])\\\n",
    "            .withColumn(out_col, F.explode(col_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b28caa-4cf1-4d26-8a1e-db7a2075ee4b",
   "metadata": {},
   "source": [
    "## Language Detection\n",
    "https://nlp.johnsnowlabs.com/2020/12/05/detect_language_21_xx.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "b9ea68fe-ee5f-4131-bbb1-d168d2bd8583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detect_language_21 download started this may take some time.\n",
      "Approx size to download 7.7 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "language_detector = PretrainedPipeline(\"detect_language_21\", lang=\"xx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73049d45-9fce-4117-8d8b-78c46b6a1a0a",
   "metadata": {},
   "source": [
    "## Spellchecker Pipeline\n",
    "https://nlp.johnsnowlabs.com/2021/03/28/spellcheck_dl_en.html<br />\n",
    "https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/7.Context_Spell_Checker.ipynb\n",
    "<br />\n",
    "While the default pipeline performs well on common English words, the pipeline requires additional training on foreign names and locations. For example, it will correct 'Bataan' (Phillipines) to 'Batman' by mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "f377f5e2-868a-4e69-9d8d-3984abbe4b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spellcheck_dl download started this may take some time.\n",
      "Approximate size to download 111.4 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Experimental training terms.\n",
    "training_terms = {\n",
    "    '_NAME_': [\n",
    "        'time', 'bataan', 'war', 'USAF', 'march', 'death', 'Tacloben',\n",
    "        'tooth', 'nicanor', 'descendent', 'memorial', 'division', 'memorial division'\n",
    "    ],\n",
    "    '_AGE_': [],\n",
    "    '_LOC_': ['philippine', 'cemetery', 'leyte'],\n",
    "    '_DATE_': [],\n",
    "    '_NUM_': []\n",
    "}\n",
    "\n",
    "def get_spellcheck_pipeline(model_name='spellcheck_dl', training_terms=[]):\n",
    "    documentAssembler = DocumentAssembler()\\\n",
    "        .setInputCol(\"text\")\\\n",
    "        .setOutputCol(\"document\")\n",
    "\n",
    "    tokenizer = RecursiveTokenizer()\\\n",
    "        .setInputCols([\"document\"])\\\n",
    "        .setOutputCol(\"token\")\\\n",
    "        .setPrefixes([\"\\\"\", \"(\", \"[\", \"\\n\"])\\\n",
    "        .setSuffixes([\".\", \",\", \"?\", \")\",\"!\", \"'s\"])\n",
    "\n",
    "    spellModel = ContextSpellCheckerModel\\\n",
    "        .pretrained(model_name)\\\n",
    "        .setInputCols(\"token\")\\\n",
    "        .setOutputCol(\"checked\")\\\n",
    "        .setErrorThreshold(4.0)\\\n",
    "        .setTradeoff(6.0)\n",
    "    \n",
    "    # Extend vocabulary of spellchecker.\n",
    "    for en_type in training_terms:\n",
    "        if re.match(r'(_NAME_|_LOC_)', en_type):\n",
    "            spellModel.updateVocabClass(en_type, training_terms[en_type], True)\n",
    "\n",
    "    finisher = Finisher()\\\n",
    "        .setInputCols(\"checked\")\n",
    "\n",
    "    return Pipeline(stages = [\n",
    "        documentAssembler,\n",
    "        tokenizer,\n",
    "        spellModel,\n",
    "        finisher\n",
    "    ])\n",
    "    \n",
    "empty_ds = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "spellcheck = LightPipeline(get_spellcheck_pipeline('spellcheck_dl', training_terms)\\\n",
    "                           .fit(empty_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "fcd8451f-d875-4ce7-9eb2-92c3fe9cdc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checked': ['Bataan', 'death', 'march', '.']}"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "spellcheck.annotate(\"Bataan death march.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "6fdd9666-bccb-41e3-ae28-1d52a46e3a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checked': ['The', 'War', 'Department', '.']}"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spellcheck.annotate(\"The Wary Department.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "1279984b-ffe3-4b99-8334-e444c8668d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checked': ['Status', 'at', 'Time', 'of', 'Death', 'Phil', 'Army']}"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spellcheck.annotate(\"Status at Tine of Death Phil Army\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "3231e8a3-7d40-428c-9439-ec05f79fe2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checked': ['Leyte', 'Island', 'Philipine', 'Islands']}"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spellcheck.annotate('Leyto Island Philippne Islands')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "02bc3fe4-fa51-4e76-8823-e164a629aa0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checked': ['Tacloben', 'Leyte', 'Island', 'Philippine', 'Islands']}"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spellcheck.annotate('Tacloben Leyte Island Philippine Islands')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "12853778-b233-4ecc-b8ea-b400c666b443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checked': ['USAF']}"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spellcheck.annotate('USAF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "171bd827-0ddd-49be-b4cc-a65e248cb72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checked': ['John', 'is', 'a', 'descendent', 'of', 'Tom', '.']}"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spellcheck.annotate('John is a decedent of Tom.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "e2a415df-66e5-458f-968e-aee7fc6af1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checked': ['nicanor', '.']}"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spellcheck.annotate('Nicanor F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "c06d6e38-13e3-43de-a280-60d89a5c1d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checked': ['VECTORIAL', 'DIVISION']}"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spellcheck.annotate('LELIORIAL DIVISION')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afb238e-312f-4df7-862c-651b9b6bd940",
   "metadata": {},
   "source": [
    "## Entity Filter\n",
    "Use to filter out undesired entity types and apply spellchecking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "9036ad51-c52f-4914-a34b-5722527f5264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_results(ner_results):\n",
    "    entities = []\n",
    "    for row in ner_results:\n",
    "        for meta, res in zip(row.metadata, row.result):\n",
    "            if 'entity' in meta \\\n",
    "                and not re.match(r'(CARDINAL|TIME|QUANTITY|DATE|ORDINAL)', meta['entity']) \\\n",
    "                and len(res) > 2:\n",
    "                entity_type = meta['entity']\n",
    "                language = language_detector.annotate(res)['language'][0]\n",
    "                if language == 'en':\n",
    "                    checked = [w for w in spellcheck.annotate(res)['checked'] if not re.match(r'[\\,\\.\\!\\?\\\"\\']', w)]\n",
    "                    checked = \" \".join(checked)\n",
    "                else:\n",
    "                    checked = res\n",
    "                entities.append({\n",
    "                    'type': entity_type, \n",
    "                    'original': res,\n",
    "                    'checked': checked,\n",
    "                    'language': language\n",
    "                })\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54972b25-ce3b-49af-a1c9-07674f434878",
   "metadata": {},
   "source": [
    "## NER Entity Meanings\n",
    "https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/NER_EN.ipynb<br />\n",
    "<table>\n",
    "    <thead><tr><th>NER</th><th>Meaning</th</tr></thead>\n",
    "    <tbody>\n",
    "        <tr><td>PERSON</td><td>People, including fictional.</td></tr>\n",
    "        <tr><td>GPE</td><td>Countries, cities, states</td></tr>\n",
    "        <tr><td>NORP</td><td>Nationalities or religious or political groups</td></tr>\n",
    "        <tr><td>DATE</td><td>Absolute or relative dates or periods</td></tr>\n",
    "        <tr><td>ORG</td><td>Companies, agencies, institutions, etc</td></tr>\n",
    "        <tr><td>CARDINAL</td><td>Numerals that do not fall under another type</td></tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "504bb242-6f6e-4cc9-a9e6-369a154fcaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ner_pipeline(model_name:str='onto_100') -> PipelineModel:\n",
    "    documentAssembler = DocumentAssembler() \\\n",
    "        .setInputCol('text') \\\n",
    "        .setOutputCol('document')\n",
    "\n",
    "    tokenizer = Tokenizer() \\\n",
    "        .setInputCols(['document']) \\\n",
    "        .setOutputCol('token')\n",
    "\n",
    "    # ner_dl and onto_100 model are trained with glove_100d, so the embeddings in\n",
    "    # the pipeline should match\n",
    "    embeddings = None\n",
    "    if (model_name == \"ner_dl\") or (model_name == 'onto_100'):\n",
    "        embeddings = WordEmbeddingsModel.pretrained('glove_100d') \\\n",
    "            .setInputCols([\"document\", 'token']) \\\n",
    "            .setOutputCol(\"embeddings\")\n",
    "\n",
    "    # Bert model uses Bert embeddings\n",
    "    elif model_name == 'ner_dl_bert':\n",
    "        embeddings = BertEmbeddings.pretrained(name='bert_base_cased', lang='en') \\\n",
    "            .setInputCols(['document', 'token']) \\\n",
    "            .setOutputCol('embeddings')\n",
    "\n",
    "    ner_model = NerDLModel.pretrained(model_name, 'en') \\\n",
    "        .setInputCols(['document', 'token', 'embeddings']) \\\n",
    "        .setOutputCol('ner')\n",
    "\n",
    "    ner_converter = NerConverter() \\\n",
    "        .setInputCols(['document', 'token', 'ner']) \\\n",
    "        .setOutputCol('ner_chunk')\n",
    "\n",
    "    pipeline = Pipeline(stages=[\n",
    "        documentAssembler, \n",
    "        tokenizer,\n",
    "        embeddings,\n",
    "        ner_model,\n",
    "        ner_converter\n",
    "    ])   \n",
    "    \n",
    "    empty_ds = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "    return pipeline.fit(empty_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f296fd-4444-4424-9597-ecf883072e5c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "<p>From the NER experiments below, ONTO 100 seems to produce the best balance of useful entities with the least noise.</p>\n",
    "<p>Methods:</p>\n",
    "<ol>\n",
    "    <li>ONTO 100</li>\n",
    "    <li>NER DL</li>\n",
    "    <li>Electra</li>\n",
    "    <li>BERT</li>\n",
    "    <li>Recognize Entities DL</li>\n",
    "    <li>Stanford NER (NLTK)</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b068540f-7cce-4eb7-abfb-61c18b9ede8e",
   "metadata": {},
   "source": [
    "## ONTO 100\n",
    "ONTO 100 seems to produce just enough entities and very little noise. It produces about twice as many entities as Electra and Stanford NER (133 vs 65 & 79) when applying filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "0c746902-ad61-4748-8b5c-1997c15b49da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pipeline from ner/onto_100_pipeline\n"
     ]
    }
   ],
   "source": [
    "onto_pipeline_path = 'ner/onto_100_pipeline'\n",
    "onto_pipeline:PipelineModel = None\n",
    "    \n",
    "if os.path.exists(onto_pipeline_path):\n",
    "    onto_pipeline = PretrainedPipeline.from_disk(onto_pipeline_path)\n",
    "    print('Loaded pipeline from', onto_pipeline_path)\n",
    "else:\n",
    "    onto_pipeline = get_ner_pipeline('onto_100')\n",
    "    onto_pipeline.save(onto_pipeline_path)\n",
    "    print('Pipeline saved to', onto_pipeline_path)\n",
    "    \n",
    "onto_df = nlp_model.transform(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "dec4e0c9-7600-4b48-a3c4-10ef2283cd38",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "onto_results = onto_df.select('ner_chunk.metadata', 'ner_chunk.result').collect()\n",
    "pd.DataFrame(filter_results(onto_results)).to_csv('ner/onto_ner_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb59482-5ed5-4244-a50a-84495003a767",
   "metadata": {},
   "source": [
    "## NER DL\n",
    "Produces 208 entities with more noise than ONTO 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "60a75cd7-e212-43c6-8161-a2e536235d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove_100d download started this may take some time.\n",
      "Approximate size to download 145.3 MB\n",
      "[OK!]\n",
      "ner_dl download started this may take some time.\n",
      "Approximate size to download 13.6 MB\n",
      "[OK!]\n",
      "Pipeline saved to ner/ner_dl_pipeline\n"
     ]
    }
   ],
   "source": [
    "ner_dl_pipeline_path = 'ner/ner_dl_pipeline'\n",
    "ner_dl_pipeline:PipelineModel = None\n",
    "    \n",
    "if os.path.exists(ner_dl_pipeline_path):\n",
    "    ner_dl_pipeline = PretrainedPipeline.from_disk(ner_dl_pipeline_path)\n",
    "    print('Loaded pipeline from', ner_dl_pipeline_path)\n",
    "else:\n",
    "    ner_dl_pipeline = get_ner_pipeline('ner_dl')\n",
    "    ner_dl_pipeline.save(ner_dl_pipeline_path)\n",
    "    print('Pipeline saved to', ner_dl_pipeline_path)\n",
    "    \n",
    "ner_dl_df = ner_dl_pipeline.transform(spark_df)\n",
    "ner_dl_results = ner_dl_df.select('ner_chunk.metadata', 'ner_chunk.result').collect()\n",
    "pd.DataFrame(filter_results(ner_dl_results)).to_csv('ner/ner_dl_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd9743e-b598-492a-b310-85bb52a93cf6",
   "metadata": {},
   "source": [
    "## NER Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a039ff0b-e489-4fff-92ec-293de3b09454",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sparknlp_display import NerVisualizer\n",
    "\n",
    "NerVisualizer().display(\n",
    "    result = ner_df.collect()[0],\n",
    "    label_col = 'ner_chunk',\n",
    "    document_col = 'document'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba102e4-35ca-4a3f-9b82-5c7e1e283594",
   "metadata": {},
   "source": [
    "## ELECTRA\n",
    "https://nlp.johnsnowlabs.com/2021/03/23/onto_recognize_entities_electra_large_en.html\n",
    "Elextra is the most minimalistic of the pretrained pipelines, pruducing very little noise but perhaps not enough entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff7964e-9add-4dc0-b01f-23c8f85ebe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "electra = PretrainedPipeline('onto_recognize_entities_electra_large', lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "29069210-68f5-4187-a7c6-fde06b20aa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = electra.fullAnnotate(spark_df, column='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6d3cde-3830-416d-9016-51b28c3badb7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "electra_results = annotations.select('entities.metadata', 'entities.result').collect()\n",
    "pd.DataFrame(filter_results(electra_results)).to_csv('ner/electra_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "e53d78fa-6ec5-448c-9969-3b9c68e190dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7b0f7c6-7b55-4654-a76a-bd01fe08d4ef",
   "metadata": {},
   "source": [
    "## BERT\n",
    "https://nlp.johnsnowlabs.com/2021/03/23/recognize_entities_bert_en.html<br/>\n",
    "BERT produces the most entities (325 after filtering) but a lot of noise/non-entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "3e960d61-86ac-4dad-aa64-c28e21f79606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recognize_entities_bert download started this may take some time.\n",
      "Approx size to download 404.6 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "bert = PretrainedPipeline('recognize_entities_bert', lang='en')\n",
    "bert_annotations = bert.fullAnnotate(spark_df, column='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f9181-b651-406b-9e3c-fd61b13affb1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_results = bert_annotations.select('entities.metadata', 'entities.result').collect()\n",
    "pd.DataFrame(filter_results(bert_results)).to_csv('ner/bert_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "fc4b09db-de85-49d9-aca1-55dde9c198f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1394d58-37a1-4801-89a5-fe7b8e6dbaa9",
   "metadata": {},
   "source": [
    "### Recognize Entities DL\n",
    "https://nlp.johnsnowlabs.com/2021/03/23/recognize_entities_dl_en.html<br />\n",
    "Identified 156 entities with filtering. Produces more noise than ONTO 100 but much less than BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "af703c79-a98b-4245-97ec-6f213672c493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recognize_entities_dl download started this may take some time.\n",
      "Approx size to download 160.1 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "recognize = PretrainedPipeline('recognize_entities_dl', lang='en')\n",
    "recognize_annotations = recognize.fullAnnotate(spark_df, column='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f478d11-2d5f-4ba8-99ee-925189f0bc0e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "recognize_results = recognize_annotations.select('entities.metadata', 'entities.result').collect()\n",
    "pd.DataFrame(filter_results(recognize_results)).to_csv('ner/recognize_entities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "d9695ce8-ca92-42b0-9ece-50d999b78c15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f12a5a6-e4ef-47d4-8b87-406af0c05590",
   "metadata": {},
   "source": [
    "### Stanford NER with NLTK\n",
    "https://www.pythonprogramming.net/named-entity-recognition-stanford-ner-tagger/<br />\n",
    "Produced 79 entities after filtering out type 'O' entities. It produced only slightly more than Electra but much less then ONTO 100 and seems too minimalistic, but produces very little noise when ignoring type 'O' objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "biological-blast",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from itertools import groupby\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "stanford_root = '../stanford-ner-4.2.0/stanford-ner-2020-11-17'\n",
    "\n",
    "st = StanfordNERTagger(stanford_root+'/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "                        stanford_root+'/stanford-ner-4.2.0.jar',\n",
    "                        encoding='utf-8')\n",
    "\n",
    "sent_tokens = sent_tokenize(sample_text)\n",
    "sents = [word_tokenize(sent) for sent in sent_tokens]\n",
    "sent_tags = st.tag_sents(sents)\n",
    "nltk_entities = []\n",
    "for sent in sent_tags:\n",
    "    for tag, chunk in groupby(sent, lambda x: x[1]):\n",
    "        en = \" \".join(w for w, t in chunk)\n",
    "        checked = \" \".join( spellcheck.annotate(en) )\n",
    "        if tag == 'O':\n",
    "            continue\n",
    "        nltk_entities.append({\n",
    "            'type': tag,\n",
    "            'original': en,\n",
    "            'checked': checked\n",
    "        })\n",
    "\n",
    "pd.DataFrame(nltk_entities).to_csv('ner/nltk_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a060fa6d-2e0a-47f2-8055-5e1c6896aad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
