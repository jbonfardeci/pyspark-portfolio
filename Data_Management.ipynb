{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL for NASA Satellite and Coast Guard GPS Data\n",
    "John Bonfardeci<br/>\n",
    "2021-01-31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Security"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access to the Raw data was provided by the customer by way of providing a publicly available S3 bucket.  In turn, the data that we make available for the customer will be in a publicly available S3 bucket to allow for easy access of the unclassified dataset.\n",
    "\n",
    "In secure environments, data at rest is encrypted using a key stored in the AWS key store. This key is made accessible via IAM Roles and Security Groups to all applications running in the secure environment that must utilize the data. This allows for storage and transfer of encrypted data within the working environment.\n",
    "\n",
    "Code that is used to build containers is run through the CI/CD pipline to be built, tested and scanned. Jenkins has plug-ins for open source scanning solutions such as Twistlok and Anchore Engine which we integrate into production environments. For this scenario, we utilized ECR with it's ability to use Clair, another open source tool to perform security scans of containers that get pushed from Jenkins.\n",
    "\n",
    "Where possible, the team utilized approved containers from DoD IronBank as the base for the containers we produce. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pull Data - NiFi (Complete)\n",
    "To pull the source data into our S3 environment, we utilized Apache NiFi.  The nifi folder in the Data Management directory of the Team Leidos VAULT git repo contains the XML template for the dataflow as well as instructions for deploying the nifi server with docker, importing the template, and making the needed configuration changes to direct the data into the appropriate bucket in your S3 instance. Below is a picture of the included template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Nifi_Template.png\" alt=\"NiFi Tempalte\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "import multiprocessing\n",
    "from multiprocessing import Process, Pool\n",
    "import fiona\n",
    "import geopandas as gpd\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO, StringIO\n",
    "import gzip\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS File System Clients and Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aws import AWSConfig\n",
    "        \n",
    "def get_boto_client():\n",
    "    cfg = AWSConfig()\n",
    "    return cfg.get_boto_client()\n",
    "\n",
    "def get_boto_resource():\n",
    "    cfg = AWSConfig()\n",
    "    return cfg.get_boto_resource()\n",
    "\n",
    "def get_fs():\n",
    "    cfg = AWSConfig()\n",
    "    return cfg.get_s3fs()\n",
    "\n",
    "def list_s3_files(folder, filtr=None):\n",
    "    return AWSConfig().list_s3_files(folder)\n",
    "\n",
    "# test\n",
    "#list_s3_files(folder='Raw/AIS/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "The following code should only be run if you are reproducing the entire ingest process, starting with NiFi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(s):\n",
    "    \"\"\"\n",
    "    Trim leading/trailing white space from string.\n",
    "    @param s<str>\n",
    "    @returns string\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return ''\n",
    "    return re.sub(r'(^\\s+|\\s+$)', '', s)\n",
    "\n",
    "def julian_to_iso(julian):\n",
    "    \"\"\"\n",
    "    Convert Epoch year and Julian data fraction to ISO date format.\n",
    "    @param julian<Int64> - Epoch year with Julian date decimal.\n",
    "    @returns string\n",
    "    \"\"\"\n",
    "    if not re.search(r'^\\d+\\.\\d+$', trim(julian)):\n",
    "        return julian\n",
    "    \n",
    "    yr = int(julian[:2])\n",
    "    yr = (2000+yr) if yr < 21 else (1900+yr)\n",
    "    day = math.floor(float(julian[2:]))\n",
    "    fraction = float('.'+julian.split('.')[1])\n",
    "    dec_hours = fraction*24\n",
    "    startdate = datetime(year=yr, month=1, day=1)\n",
    "    startdate += timedelta(days=day)\n",
    "    startdate += timedelta(hours=dec_hours)\n",
    "    return startdate.isoformat()\n",
    "\n",
    "def execute_workers(jobs, process_chunk, num_processes=0):\n",
    "    \"\"\"\n",
    "    Execute multiple processes on a list of jobs.\n",
    "    @param jobs<List<any>> - Execute over a list of objects.\n",
    "    @param process_chunk<function> - A closure function.\n",
    "    @param num_processes<Int> - Number of cores to use. Default = 0 = all cores available.\n",
    "    \"\"\"\n",
    "    if num_processes == 0:\n",
    "        num_processes = multiprocessing.cpu_count()\n",
    "        \n",
    "    print('%i worker processes available.\\n' % (num_processes))\n",
    "            \n",
    "    list_len = len(jobs)\n",
    "    n = math.ceil(list_len/num_processes)\n",
    "    \n",
    "    chunks = [jobs[i:i + n] for i in range(0, list_len, n)]\n",
    "    print('%i files will be divided among %i workers.\\n' % (list_len, len(chunks)))\n",
    "    procs = [] \n",
    "    \n",
    "    for chunk in chunks:\n",
    "        p = Process(target=process_chunk, args=(chunk,))\n",
    "        procs.append(p)\n",
    "        p.start()\n",
    "\n",
    "    for proc in procs:\n",
    "        proc.join()\n",
    "\n",
    "def unzip_s3_files(folder='', out_folder='extracted', filtr=None, num_processes=0):\n",
    "    \"\"\"\n",
    "    Unzip all files in a given AWS S3 folder. \n",
    "    Note: Requires AWS access keys stored in .aws directory of machine.\n",
    "    \n",
    "    @param folder<str> - The source folder to read from. Default is root folder.\n",
    "    @param out_folder<str> - Where to write unzipped files. Default ='extracted'\n",
    "    @param s3_bucket<str> - The AWS S3 bucket name. Default='af-vault'\n",
    "    @param region_name<str> - The AWS region name for the S3 bucket. Default='us-gov-east-1'\n",
    "    @param filtr<function> - A filter function for granular file path pattern matching. Default=None.\n",
    "    \"\"\"\n",
    "    aws = AWSConfig()\n",
    "    bucket_name = aws.s3_bucket\n",
    "    s3r = aws.get_boto_resource()\n",
    "    afvbucket = aws.get_bucket()\n",
    "    files = afvbucket.objects.filter(Prefix=folder)\n",
    "    print(files)\n",
    "    zip_files = [f for f in files if str(f.key).endswith('.zip')]\n",
    "    \n",
    "    if filtr:\n",
    "        zip_files = [f for f in zip_files if filtr(f.key)]\n",
    "        \n",
    "    if len(zip_files) == 0:\n",
    "        print('The zip file list is empty. Stopping.')\n",
    "        \n",
    "    extracted_files = [f.key for f in files if not str(f.key).endswith('.zip')]  \n",
    "    print('%i zip files were found. Extracting.' % (len(zip_files)))\n",
    "    \n",
    "    def process_chunk(zip_files):\n",
    "        for item in zip_files:\n",
    "            itemKey = item.key\n",
    "            print('Reading %s...' % (itemKey))\n",
    "\n",
    "            zip_obj = s3r.Object(bucket_name=bucket_name, key=itemKey)\n",
    "            buffer = BytesIO(zip_obj.get()[\"Body\"].read())\n",
    "            z = ZipFile(buffer)\n",
    "\n",
    "            for file_path in z.namelist():\n",
    "                fname = os.path.basename(file_path)\n",
    "                subfolder = file_path.split('/')[0]\n",
    "                dest = str.format('{0}/{1}/{2}/{3}', folder, out_folder, subfolder, fname)\n",
    "                if dest in extracted_files:\n",
    "                    print('%s already exists. Skipping.' % (dest))\n",
    "                    continue\n",
    "\n",
    "                print('Unzipping %s to %s...' % (file_path, dest))\n",
    "                s3r.meta.client.upload_fileobj(\n",
    "                    z.open(file_path),\n",
    "                    Bucket=bucket_name,\n",
    "                    Key=dest\n",
    "                )\n",
    "                print('Unzipped %s.' % (file_path))      \n",
    "            print('Closed %s.' % (itemKey))\n",
    "    \n",
    "    execute_workers(jobs=zip_files, process_chunk=process_chunk, num_processes=num_processes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Convert AIS Data\n",
    "1. Read AIS files from S3.\n",
    "2. For each AIS file, unzip and read GDB layers in memory stream via Fiona.\n",
    "3. Convert GDB layers to CSV files via GeoPandas.\n",
    "4. Get and store min/max BaseDateTime from each Broadcast layers.\n",
    "5. Write CSV layers back to S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Extract AIS CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ais_filter = lambda filename: re.search(r'\\/AIS_\\d{4}.*.zip$', filename)\n",
    "unzip_s3_files(folder='Raw/AIS', filtr=ais_filter, num_processes=0)\n",
    "print('AIS files extracted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Extract and Convert AIS GDB Files\n",
    "Extracting the layers from each GDB file and convertng to CSV takes the longest of all ETL tasks.<br/>\n",
    "We believe Geopandas is the bottlneck here. It may be possible to write our own converter from the Fiona obejct arrays to CSV files.<br/>This process can take 8 hours or more, but this step will only run the first time since it will skip GDB layer files that have already been extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def convert_gdb_file(path, layers, out_folder, sample=False):\n",
    "    \"\"\"\n",
    "    Unzip and extract all layers from a GDB file in an S3 bucket.\n",
    "    \n",
    "    @param path<str> - File path in S3.\n",
    "    @layers List[str] - list of layer names to extract.\n",
    "    @param out_folder<str> - The destination subfolder to extract layers to.\n",
    "    @param sample<Bool> - If True, writes only the first 10 rows of each layer. use for testing only. Default = False.\n",
    "    \"\"\"\n",
    "    date_range = []\n",
    "    fs = get_fs()\n",
    "    \n",
    "    def _change_filename(s, ext):\n",
    "        f = s.split('.')[0]\n",
    "        return str.format('{0}.{1}', f, ext) \n",
    "    \n",
    "    fname = os.path.basename(path)\n",
    "    gdb_fname = _change_filename(fname, 'gdb')\n",
    "    print('Reading: %s...\\n' % (gdb_fname))\n",
    "\n",
    "    with fs.open(path, 'rb') as f:\n",
    "        print('%s opened. Reading buffer into memory stream...\\n' % (path))\n",
    "        buffer = bytes(f.read())\n",
    "        with fiona.io.ZipMemoryFile(buffer) as z:\n",
    "            z.seek(0)\n",
    "            for layer in layers: \n",
    "                print('Reading layer %s > %s...\\n' % (gdb_fname, layer))\n",
    "                clean_layer_name = layer if not re.search(r'\\_', layer) else layer.split('_')[-1]\n",
    "                out_filename = gdb_fname.replace('.gdb', str.format('.{0}.csv', clean_layer_name))\n",
    "                csv_fname = str.format(\"{0}/{1}\", out_folder, out_filename)\n",
    "                \n",
    "                # Only process the file if it doesn't already exist.\n",
    "                if fs.exists(csv_fname):\n",
    "                    print('%s already exists. Skipping.\\n' % (csv_fname))\n",
    "                    continue\n",
    "                \n",
    "                with z.open(gdb_fname, driver='FileGDB', layer=layer) as collection:\n",
    "                    print('Converting %s and layer %s...\\n' % (gdb_fname, layer))\n",
    "                    if sample:\n",
    "                        collection = collection[:10]\n",
    "                        \n",
    "                    df = gpd.GeoDataFrame.from_features(collection)\n",
    "                    # Writes CSV directly to S3 folder with Fiona and Boto3 under the hood.\n",
    "                    df.to_csv(csv_fname, index=None)\n",
    "                    \n",
    "                    print('%s written.\\n' % (csv_fname))\n",
    "                    \n",
    "            z.close()\n",
    "            \n",
    "        print('%s closed.\\n' % (gdb_fname))\n",
    "        f.close()\n",
    "\n",
    "def convert_gdb_files(filelist, out_folder, num_processes=0, sample=False):\n",
    "    \"\"\"\n",
    "    Unzip and extract all layers from a list of GDB files in an S3 bucket.\n",
    "    \n",
    "    @param filelist<List[str]> - List of file paths in S3.\n",
    "    @param out_folder<str> - The destination subfolder to extract layers to.\n",
    "    @param num_processes<int> - The number of worker processes to utilized. Default=0 which means all CPUs will be used.\n",
    "    @param sample<Bool> - If True, writes only the first 10 rows of each layer. use for testing only. Default = False.\n",
    "    @returns Int - Files extarcted.\n",
    "    \"\"\"\n",
    "    print('Converting %i GDB files...\\n' % (len(filelist)))\n",
    "    \n",
    "    gdb_definitions = {\n",
    "        'Zone10_2009_01': ['Broadcast', 'Vessel', 'Voyage']\n",
    "        , 'Zone10_2010_01': ['Voyage', 'Vessel', 'BaseStations', 'Broadcast', 'AttributeUnits']\n",
    "        , 'Zone10_2011_01': ['Broadcast', 'Vessel', 'Voyage']\n",
    "        , 'Zone10_2012_01': ['Broadcast', 'Vessel', 'Voyage']\n",
    "        , 'Zone10_2013_01': ['Zone10_2013_01_Broadcast', 'Zone10_2013_01_Vessel', 'Zone10_2013_01_Voyage']\n",
    "        , 'Zone10_2014_01': ['Zone10_2014_01_Broadcast', 'Zone10_2014_01_Vessel', 'Zone10_2014_01_Voyage']\n",
    "    }\n",
    "    \n",
    "    def process_chunk(filelist):\n",
    "        for path in filelist:\n",
    "            fname = os.path.basename(path).split('.')[0]\n",
    "            layers = layers = gdb_definitions[fname]\n",
    "            convert_gdb_file(path, layers, out_folder, sample)\n",
    "    \n",
    "    execute_workers(jobs=filelist, process_chunk=process_chunk, num_processes=num_processes) \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    aws = AWSConfig()\n",
    "    filtr = lambda filename: re.search(r'\\/Zone\\d+\\_.*\\.zip$', filename)\n",
    "    filelist = list_s3_files(folder='Raw/AIS', filtr=filtr)\n",
    "    filelist = [f for f in filelist if f.endswith('.zip') and not re.search(r'AIS_', f)]\n",
    "    print('Found %i files to extract.' % (len(filelist)))\n",
    "    \n",
    "    if len(filelist) == 0:\n",
    "        print('The AIS GDB file list is empty. Stopping.\\n')\n",
    "    else:\n",
    "        print('Extracting GDB layers to CSV...\\n')\n",
    "        out_folder = str.format('s3://{0}/Raw/AIS/extracted', aws.s3_bucket)\n",
    "        convert_gdb_files(filelist=filelist, out_folder=out_folder, num_processes=0, sample=False)\n",
    "        print('GDB layers were extracted to CSV.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert TLE Data\n",
    "1. Read TLE files from S3.\n",
    "2. For each TLE file, unzip in memory stream via S3FS.\n",
    "3. Convert two-line data to single row, extracting the satellite ID and datetime to new columns.\n",
    "    <br/>Schema: SatID | BaseDateTime | Line 1 | Line 2\n",
    "4. Filter TLE rows to match date ranges in AIS data: 2008-2017 in month of January.\n",
    "5. Write converted TLE files back to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tle_line(line):\n",
    "    \"\"\"\n",
    "    Clean TLE data row.\n",
    "    @param line<str> - raw line of TLE text\n",
    "    @returns str\n",
    "    \"\"\"\n",
    "    nl = '\\n' if re.search(r'\\n', line) and re.search(r'^2\\s', line) else ''\n",
    "    line = trim(line)\n",
    "    \n",
    "    row = []\n",
    "    if re.search('^1\\s', line):  \n",
    "        line = re.sub(r'\\\\', '', line)\n",
    "        \n",
    "        # col 1 - SatId\n",
    "        satId = trim( re.sub(r'\\D', '', line[2:8]) )\n",
    "        row.extend([satId, '|'])\n",
    "        \n",
    "        # col 5 - EpochYear, Convert Julian date to ISO: \n",
    "        julian_date = trim(line[17:32])\n",
    "        if julian_date:\n",
    "            julian_date = julian_to_iso(julian_date)\n",
    "         \n",
    "        row.extend([julian_date, '|'])     \n",
    "        row.extend([line, '|'])\n",
    "    else:\n",
    "        row.append(line)\n",
    "        \n",
    "    return ''.join(row) + nl\n",
    "\n",
    "def clean_tle_file(path, s3_bucket, filtr=None):\n",
    "    \"\"\"\n",
    "    Clean TLE file. Each data row of a TLE (two-line-element) file is written on two lines.\n",
    "    This function combines both lines into a delimited single row \n",
    "        with new columns for satellite ID and timestamp.\n",
    "    \"\"\"\n",
    "    print('Reading %s...\\n' % (path))\n",
    "    \n",
    "    tmp = path.split('/')\n",
    "    fname = tmp[-1]\n",
    "    filename = 'Raw/TLE/cleaned/'+fname\n",
    "    \n",
    "    fs = get_fs()\n",
    "    \n",
    "    if fs.exists( str.format('s3://{0}/{1}', s3_bucket, filename) ):\n",
    "        print(filename, 'already exists. Skipping.\\n')\n",
    "        return True\n",
    "    \n",
    "    # Memory stream lines will be written to.\n",
    "    mem_stream = StringIO()\n",
    "    \n",
    "    # Open file directly from AWS S3 bucket and read line by line.\n",
    "    with fs.open(path, mode='rb') as file:\n",
    "        print('Opening %s...\\n' % (path))\n",
    "        \n",
    "        is_valid = False\n",
    "        while True:\n",
    "            line = file.readline()\n",
    "            if not line:\n",
    "                # End of file. Close stream and exit loop.\n",
    "                file.close()\n",
    "                print('Cleaned %s\\n' % (filename))\n",
    "                break\n",
    "\n",
    "            str_line = line.decode() # Decode binary line to string.\n",
    "            cleaned_line = clean_tle_line(str_line)\n",
    "                    \n",
    "            if filtr:\n",
    "                # Run filter function on line, if present. Returns True|False.\n",
    "                is_line_1 = re.search(r'^1\\s+', str_line) # Line one of TLE data starts with \"1 ...\"\n",
    "                if is_line_1:\n",
    "                    if filtr(cleaned_line):\n",
    "                        mem_stream.write(cleaned_line)\n",
    "                        # If the filter function determines line 1 is valid, \n",
    "                        # give go-ahead to write line 2 on the next pass.\n",
    "                        is_valid = True\n",
    "                    else: \n",
    "                        # Line 1 is invalid. Skip line 2 on the next pass.\n",
    "                        is_valid = False\n",
    "                elif is_valid:\n",
    "                    # Write line 2 when line is valid on the previous iteration.\n",
    "                    mem_stream.write(cleaned_line)\n",
    "            else:\n",
    "                # No filter function present. Write line.\n",
    "                mem_stream.write(cleaned_line)\n",
    "                \n",
    "        file.close()\n",
    "        print('%s Closed.\\n' % (path))\n",
    "            \n",
    "    contents = mem_stream.getvalue()\n",
    "    s3 = get_boto_client()\n",
    "    print('Writing %s...\\n' % (filename))\n",
    "    res = s3.put_object(Body=contents, Bucket=s3_bucket, Key=filename)\n",
    "    s3 = None\n",
    "    mem_stream.close()\n",
    "    \n",
    "    print('%s written and stream closed.\\n' % (filename))\n",
    "    status = res['ResponseMetadata']['HTTPStatusCode']\n",
    "    \n",
    "    print('HTTP Status Code = %i.\\n' % (status))\n",
    "    if status == 200:\n",
    "        print('Writing %s succeeded!\\n' % (filename))\n",
    "    else:\n",
    "        print('Writing %s failed!\\n' % (filename))\n",
    "    \n",
    "def clean_tle_files(filelist, s3_bucket, filtr=None, num_processes=0):\n",
    "    \"\"\"\n",
    "    Clean TLE data over N logical cores. Save as pipe delimitted datasets.\n",
    "    @param filelist<List[str]> - List of files in S3 to clean.\n",
    "    @param num_processes<int> - The number of logical CPU cores on a system. '0' = all cores.\n",
    "    \"\"\"\n",
    "    def process_chunk(filelist):\n",
    "        for path in filelist:\n",
    "            clean_tle_file(path=path, s3_bucket=s3_bucket, filtr=filtr)\n",
    "\n",
    "    execute_workers(jobs=filelist, process_chunk=process_chunk, num_processes=num_processes)  \n",
    "    \n",
    "if __name__ == '__main__': \n",
    "    aws = AWSConfig()\n",
    "    s3_bucket = aws.s3_bucket\n",
    "    \n",
    "    def _filtr(filename):\n",
    "        \"\"\"\n",
    "        Filter 2008-2018 only since this matches AIS data.\n",
    "        \"\"\"\n",
    "        tmp = filename.split('/')\n",
    "        fname = tmp[-1]\n",
    "        n = re.sub(r'\\D', '', fname)\n",
    "        if len(n) == 0:\n",
    "            return False\n",
    "        yr = int(n)\n",
    "        return yr > 2007 and yr < 2019\n",
    "     \n",
    "    def _date_filter(line):\n",
    "        \"\"\"\n",
    "        Filter on year between 2009 and 2017 with Dec./Feb. overlap by one day.\n",
    "            to match AIS date ranges, 2008-12-31 2017-01-31\n",
    "        \"\"\"\n",
    "        vals = line.split('|')\n",
    "        # e.g. 2004-08-18T05:19:09.399648\n",
    "        if len(vals) > 2 and re.search(r'^\\d{4}\\-\\d{2}\\-\\d{2}', vals[1]):\n",
    "            ds = vals[1]\n",
    "            dt = datetime.fromisoformat(ds)\n",
    "            month = dt.month\n",
    "            day = dt.day\n",
    "            yr = dt.year\n",
    "            return (yr >= 2008 and yr <= 2018) \\\n",
    "                and (month==1 or (month==12 and day == 31) or (month==2 and day == 1))\n",
    "        return True\n",
    "    \n",
    "    s3_folder = 'Raw/TLE'\n",
    "    filelist = list_s3_files(folder=s3_folder, filtr=_filtr)\n",
    "    #filelist = ['s3://af-vault/Raw/TLE/test.txt']\n",
    "    \n",
    "    if len(filelist) == 0:\n",
    "        print('The TLE file list is empty. Stopping.')\n",
    "    else: \n",
    "        clean_tle_files(filelist=filelist, s3_bucket=s3_bucket, filtr=_date_filter, num_processes=0)\n",
    "        print('%i TLE files were cleaned.' % (len(filelist)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
